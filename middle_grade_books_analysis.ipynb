{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import bs4 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.goodreads.com/shelf/show/middle-grade\"\n",
    "source = urllib.request.urlopen(url).read()\n",
    "soup = bs4.BeautifulSoup(source, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book number:45121\n",
      "Number of web pages: 903\n"
     ]
    }
   ],
   "source": [
    "#Extract the total number of books in middle-grade shelf\n",
    "book_num_string = soup.find('span', class_='smallText').string.rstrip()\n",
    "#print(book_num_string)\n",
    "book_num_fields = book_num_string.split(' ')[-1].split(',')\n",
    "book_num = int(\"\".join(book_num_fields))\n",
    "print(\"book number:\" + str(book_num))\n",
    "\n",
    "#Compute the number of pages we need to crawl to get all books\n",
    "num_per_page = 50\n",
    "total_pages = int(book_num / num_per_page) + 1\n",
    "print(\"Number of web pages: \" + str(total_pages))\n",
    "\n",
    "domain_name = \"https://www.goodreads.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "def extract_book_link_one_page(page_num):\n",
    "    \"\"\"Extract the book links in the page with index page_num\"\"\"\n",
    "    \n",
    "    url = \"https://www.goodreads.com/shelf/show/middle-grade?page=\" + str(page_num)\n",
    "    html_page = urllib.request.urlopen(url).read()\n",
    "    soup = bs4.BeautifulSoup(html_page, 'html.parser')\n",
    "    \n",
    "    #Extract links for books\n",
    "    links_of_one_page = [domain_name + a.get(\"href\") for a in soup.find_all('a', class_= \"bookTitle\")]\n",
    "    return links_of_one_page\n",
    "\n",
    "links = extract_book_link_one_page(1)\n",
    "print(len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.goodreads.com/book/show/28187.The_Lightning_Thief\n"
     ]
    }
   ],
   "source": [
    "print(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a pandas DataFrame to store books information scraped from webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_names = ['Title', 'Author', 'Ave_rating', 'Rating_cnt', 'Review_cnt', 'Price', 'Year', 'Pages', 'Publisher']\n",
    "column_names = ['Title', 'Author', 'Ave_rating', 'Rating_cnt', 'Review_cnt', 'Year', 'Pages', 'Publisher']\n",
    "books_df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_from_amazon(amazon_link):\n",
    "    \"\"\"Get the price from the amazon page\"\"\"\n",
    "    amazon_page = urllib.request.urlopen(amazon_link).read()\n",
    "    soup = bs4.BeautifulSoup(amazon_page, 'html.parser')\n",
    "    \n",
    "    price = soup.find('span', class_=\"a-size-base a-color-price a-color-price\")\n",
    "    print(price)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Lightning Thief',\n",
       " 'Rick Riordan',\n",
       " 4.24,\n",
       " 1681440,\n",
       " 52520,\n",
       " '2006',\n",
       " 377,\n",
       " 'Disney Hyperion Books']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_one_book(book_link):\n",
    "    \"\"\"parse one book and store information in pandas object\"\"\"\n",
    "    book_page = urllib.request.urlopen(book_link).read()\n",
    "    soup = bs4.BeautifulSoup(book_page, 'html.parser')\n",
    "    \n",
    "    title = soup.find(id=\"bookTitle\").string.strip()\n",
    "    author = soup.find('a', class_= \"authorName\").find('span').string\n",
    "    ave_rating = float(soup.find(\"span\", itemprop=\"ratingValue\").string)\n",
    "    rating_cnt = int(soup.find(\"meta\", itemprop = \"ratingCount\").get(\"content\"))\n",
    "    review_cnt = int(soup.find(\"meta\", itemprop = \"reviewCount\").get(\"content\"))\n",
    "    \n",
    "   # amazon_link = domain_name + soup.find(id=\"buyButton\").get(\"href\")\n",
    "   # price = get_price_from_amazon(amazon_link)\n",
    "    \n",
    "    details = str(soup.find(id=\"details\").find_all(\"div\")[1])\n",
    "    year = re.findall(\"\\d{3,4}\", details)[0]\n",
    "    publisher_line = details.split(\"\\n\")[3]\n",
    "    m = re.search(r\"by\\s+(.+)\", publisher_line);\n",
    "    publisher = m.group(1)\n",
    "    \n",
    "    pages = int(soup.find(\"span\", itemprop = \"numberOfPages\").string.split(\" \")[0])\n",
    "    \n",
    "    #book = [title, author, ave_rating, rating_cnt, review_cnt, price, year, pages, publisher]\n",
    "    book = [title, author, ave_rating, rating_cnt, review_cnt, year, pages, publisher]\n",
    "    return book\n",
    "     \n",
    "\n",
    "parse_one_book(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the books information of one page in a pandas dataframe and append them to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "['The Lightning Thief', 'Rick Riordan', 4.24, 1681440, 52520, '2006', 377, 'Disney Hyperion Books']\n",
      "[\"Harry Potter and the Sorcerer's Stone\", 'J.K. Rowling', 4.46, 5848097, 93982, '1997', 320, 'Scholastic Inc']\n",
      "['Harry Potter and the Chamber of Secrets', 'J.K. Rowling', 4.41, 2267965, 43326, '1999', 341, 'Arthur A. Levine Books / Scholastic Inc.']\n",
      "['Wonder', 'R.J. Palacio', 4.45, 577657, 59596, '2012', 315, 'Alfred A. Knopf']\n",
      "['The Sea of Monsters', 'Rick Riordan', 4.24, 633376, 23028, '2006', 279, 'Hyperion Books']\n",
      "['Harry Potter and the Prisoner of Azkaban', 'J.K. Rowling', 4.55, 2316208, 45509, '2004', 435, 'Scholastic Inc.']\n",
      "[\"The Titan's Curse\", 'Rick Riordan', 4.35, 582789, 17749, '2007', 320, 'Puffin Books']\n",
      "['The Battle of the Labyrinth', 'Rick Riordan', 4.39, 613725, 16262, '2008', 361, 'Hyperion Books for Children']\n",
      "['The Last Olympian', 'Rick Riordan', 4.51, 569812, 20025, '2009', 381, 'Disney-Hyperion Books']\n",
      "['A Wrinkle in Time', \"Madeleine L'Engle\", 4.01, 868643, 31038, '2017', 218, 'Square Fish']\n",
      "['The Bad Beginning', 'Lemony Snicket', 3.93, 343045, 15095, '1999', 176, 'Scholastic, Inc.']\n",
      "['Harry Potter and the Goblet of Fire', 'J.K. Rowling', 4.55, 2168131, 38251, '2002', 734, 'Scholastic']\n",
      "['Harry Potter and the Order of the Phoenix', 'J.K. Rowling', 4.49, 2122600, 35118, '2004', 870, 'Scholastic Inc.']\n",
      "['Holes', 'Louis Sachar', 3.95, 859406, 18225, '2000', 233, 'Scholastic']\n",
      "['The Giver', 'Lois Lowry', 4.12, 1512443, 60013, '2006', 208, 'Ember']\n",
      "['Harry Potter and the Deathly Hallows', 'J.K. Rowling', 4.63, 2202522, 58728, '2007', 759, 'Arthur A. Levine Books / Scholastic Inc.']\n",
      "['Harry Potter and the Half-Blood Prince', 'J.K. Rowling', 4.56, 2070718, 33240, '2006', 652, 'Scholastic Inc.']\n",
      "['The Graveyard Book', 'Neil Gaiman', 4.13, 383743, 27833, '2008', 307, 'HarperCollins']\n",
      "['Coraline', 'Neil Gaiman', 4.05, 408705, 16989, '2006', 162, 'William Morrow Paperbacks']\n",
      "['The Lion, the Witch and the Wardrobe', 'C.S. Lewis', 4.21, 1864039, 18306, '2005', 206, 'HarperCollins Publishers']\n",
      "['The Girl Who Drank the Moon', 'Kelly Barnhill', 4.19, 31430, 6405, '2016', 388, 'Algonquin Young Readers']\n",
      "['The Lost Hero', 'Rick Riordan', 4.35, 546758, 19403, '2010', 557, 'Disney-Hyperion Books']\n",
      "['The One and Only Ivan', 'Katherine Applegate', 4.26, 107512, 12245, '2012', 307, 'HarperCollins']\n",
      "['When You Reach Me', 'Rebecca Stead', 4.08, 77276, 10811, '2009', 199, 'Wendy Lamb Books']\n",
      "['The School for Good and Evil', 'Soman Chainani', 4.01, 50987, 6064, '2018', 544, 'HarperCollins; Reprint edition']\n",
      "['Bridge to Terabithia', 'Katherine Paterson', 3.99, 392815, 10916, '2009', 190, 'HarperCollins']\n",
      "['Ella Enchanted', 'Gail Carson Levine', 3.98, 365071, 9437, '1998', 232, 'Scholastic Books']\n",
      "['Matilda', 'Roald Dahl', 4.3, 517409, 13664, '1998', 240, 'Puffin Books']\n",
      "['George', 'Alex Gino', 4.04, 22434, 4549, '2015', 213, 'Scholastic Inc.']\n",
      "['The War That Saved My Life', 'Kimberly Brubaker Bradley', 4.49, 40485, 6783, '2015', 316, 'Dial Books']\n",
      "[\"Charlotte's Web\", 'E.B. White', 4.16, 1239428, 15590, '2001', 184, 'HarperCollinsPublishers']\n",
      "['The Reptile Room', 'Lemony Snicket', 3.98, 161884, 5907, '1999', 192, 'Scholastic, Inc.']\n",
      "['Nevermoor: The Trials of Morrigan Crow', 'Jessica  Townsend', 4.4, 16338, 3549, '2017', 465, 'Little, Brown Books for Young Readers']\n",
      "['Artemis Fowl', 'Eoin Colfer', 3.84, 441681, 9731, '2003', 396, 'Disney-Hyperion']\n",
      "['City of Ghosts', 'Victoria Schwab', 3.96, 16171, 3586, '2018', 272, 'Scholastic']\n",
      "['The Red Pyramid', 'Rick Riordan', 4.08, 277898, 12052, '2010', 516, 'Disney-Hyperion']\n",
      "['The Son of Neptune', 'Rick Riordan', 4.44, 360730, 13852, '2011', 521, 'Disney-Hyperion Books']\n",
      "['The Iron Trial', 'Holly Black', 3.95, 53197, 6237, '2014', 295, 'Scholastic Press']\n",
      "['The Sword of Summer', 'Rick Riordan', 4.26, 120798, 10909, '2015', 491, 'Disney - Hyperion Books']\n",
      "['The Mark of Athena', 'Rick Riordan', 4.46, 393921, 14800, '2012', 586, 'Disney-Hyperion Books']\n",
      "['The Mysterious Benedict Society', 'Trenton Lee Stewart', 4.15, 108389, 9408, '2008', 497, 'Little, Brown Books for Young Readers']\n",
      "['The Invention of Hugo Cabret', 'Brian Selznick', 4.24, 147859, 14234, '2007', 525, 'Scholastic Press']\n",
      "['Brown Girl Dreaming', 'Jacqueline Woodson', 4.14, 50630, 7520, '2014', 337, 'Nancy Paulsen Books']\n",
      "['Number the Stars', 'Lois Lowry', 4.13, 387206, 11961, '1998', 137, 'Laurel Leaf']\n",
      "['Furthermore', 'Tahereh Mafi', 3.93, 10033, 2641, '2016', 406, 'Dutton Books for Young Readers']\n",
      "['The Wide Window', 'Lemony Snicket', 3.93, 142125, 4369, '2000', 214, 'HarperCollins Publishers']\n",
      "['The House of Hades', 'Rick Riordan', 4.55, 215164, 13216, '2013', 597, 'Disney-Hyperion Books']\n",
      "['A Monster Calls', 'Patrick Ness', 4.37, 170769, 30027, '2015', 237, 'Walker Books']\n",
      "['The Hidden Oracle', 'Rick Riordan', 4.28, 78931, 8952, '2016', 376, 'Disney-Hyperion']\n",
      "['Pax', 'Sara Pennypacker', 4.04, 22210, 3961, '2016', 224, 'Balzer + Bray']\n"
     ]
    }
   ],
   "source": [
    "column_names = ['Title', 'Author', 'Ave_rating', 'Rating_cnt', 'Review_cnt', 'Year', 'Pages', 'Publisher']\n",
    "\n",
    "\n",
    "def scrape_books(total_pages, export_csv_file):\n",
    "    \"\"\"Scrape books from all pages\"\"\"\n",
    "    with open(export_csv_file, 'a') as write_obj:\n",
    "        writer = csv.writer(write_obj)\n",
    "        writer.writerow(column_names)\n",
    "    for page_num in np.arange(1, (total_pages + 1)):\n",
    "        print(\"Processing page \" + str(page_num) + \"...\")\n",
    "        with open(export_csv_file, 'a') as write_obj:\n",
    "            writer = csv.writer(write_obj)\n",
    "    \n",
    "            links_of_one_page = extract_book_link_one_page(page_num)\n",
    "            for link in links_of_one_page:\n",
    "                book_info = parse_one_book(link)\n",
    "                print(book_info)\n",
    "                writer.writerow(book_info)\n",
    "            \n",
    "            \n",
    "\n",
    "export_csv_file = \"/Users/hao/code/GoodReadsBooksAnalysis/middle_grade_books.page1.csv\"\n",
    "scrape_books = scrape_books(1, export_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Author, Ave_rating, Rating_cnt, Review_cnt, Year, Pages, Publisher]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
